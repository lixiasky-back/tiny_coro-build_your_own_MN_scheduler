# üìñ Building a High-Performance C++20 M:N Scheduler from Scratch: The Complete Guide

> **Foreword**:
> Welcome to the deep waters of systems programming.
> You may have heard of Go's Goroutines or Rust's Tokio. They are fast because they don't rely on heavy operating system threads for every concurrent task. Instead, they implement an **M:N Scheduling Model**:
> * **M (User Tasks)**: Thousands of lightweight coroutines (user-space tasks).
> * **N (Kernel Threads)**: Only a few (usually equal to the number of CPU cores) physical threads.
>
> This guide will take you step-by-step through building an asynchronous runtime framework, `tiny_coro`, like playing with LEGO blocks.

---
# üìñ Chapter 0: The Warm-up Before the Deep Dive

> **Introduction**:
> To write a scheduler by hand, you must first reach a "tacit understanding" with the compiler.
> Coroutines are essentially state machines; the transfer of tasks relies on ownership, and squeezing out performance requires precise control over memory movement.

---

## 0.1 The Left vs. Right Battle: Lvalue and Rvalue

This is the cornerstone of C++ performance optimization. Simply put:
* **Lvalue**: An object that has a name, a fixed address, and whose address can be taken. It is "persistent."
* **Rvalue**: An object that is nameless, temporary, and about to be destroyed (such as literals or temporary values returned by functions).

```cpp
int a = 10;          // 'a' is an lvalue, '10' is an rvalue
int b = a + 5;       // 'b' is an lvalue, '(a + 5)' produces an rvalue
```

**Why distinguish them?**
Because if I know an object is an "rvalue," it means it is about to die. Since it's dying, I can **steal** its resources instead of clumsily copying them.

---

## 0.2 The Truth of Movement: What does `std::move` actually do?
> **The Essence of `std::move`: It's not actually a pointer swap.**

Many mistakenly believe `move` is a pointer swap or some kind of magic. **No, the essence of `std::move` is just one thing: a type cast.**

Its sole purpose is to force an "lvalue" to be recognized as an "rvalue," thereby triggering a "move constructor."

### Stripping down the Pseudo-code implementation
The underlying implementation of `std::move` is simply removing the reference and casting it to an rvalue reference:
```cpp
template <typename T>
decltype(auto) my_move(T&& arg) {
    // Force cast to rvalue reference type T&&
    return static_cast<std::remove_reference_t<T>&&>(arg);
}
```

### Why isn't it a pointer swap?
`move` only grants you the "permission to steal." The actual "stealing" happens in the **move constructor** you write.
```cpp
class Buffer {
    char* data;
public:
    // Move Constructor: This is where the "swap" or "takeover" actually happens
    Buffer(Buffer&& other) noexcept {
        this->data = other.data;  // Steal the pointer
        other.data = nullptr;     // Wipe the traces of the original owner (prevent double free)
    }
};
```
If you `move` a simple `int`, a copy still occurs because an `int` has no pointers to steal. Therefore, **move optimization is only effective for complex objects holding resources like heap memory.**

---

## 0.3 State Machines: The Skeleton of Coroutines

The reason a coroutine can "pause" is that the compiler breaks your function logic into a **state machine**.

### 1. Conceptual Understanding: A Simple Lightbulb State Machine
A state machine is: what state am I in now, what input did I receive, and which state do I jump to next.
```cpp
enum class State { OFF, ON };
State current = State::OFF;

void press_switch() {
    if (current == State::OFF) current = State::ON;
    else current = State::OFF;
}
```
Now that you understand a simple state machine, the following "complex" version might be confusing. We first need to understand the official C++20 coroutine mechanics. Don't worry, we don't need to read the cryptic manuals. Jump here to check it out, then come back:
[C++20 Coroutines Deciphered](docs/coruntime.md)

### 2. The "Complex" State Machine Translated from Coroutines
When you write a coroutine, the compiler translates it into a structure similar to the one below. Notice how it enters different branches via `resume`:



```cpp
// Your original code
Task demo() {
    int x = 1;
    co_await std::suspend_always{}; // Pause 1
    x++;
    co_await std::suspend_always{}; // Pause 2
}

// Pseudo-code generated by the compiler (simplified)
struct demo_state_machine {
    int state = 0; // State machine control variable
    int x;         // Local variables become class members to "save progress" across pauses

    void resume() {
        switch (state) {
        case 0:
            x = 1;
            state = 1; // Prepare to jump to the next state
            return;    // Pause, return to caller
        case 1:
            x++;
            state = 2;
            return;    // Pause, return to caller
        case 2:
            // End
            return;
        }
    }
};
```

---

## 0.4 Ownership: Who Manages the Memory?

Ownership determines: **When the code runs to this point, who is responsible for releasing this memory?** (i.e., who is the "destroyer")

### A Visual from Rust
Rust enforces ownership transfer by default, which helps us understand `std::unique_ptr` in C++.
```rust
let s1 = String::from("hello");
let s2 = s1; // Ownership transferred! s1 is now invalid and cannot be accessed
```

### Corresponding Concept in C++
In `tiny_coro`, ownership is vital. When a `Task` is `spawn`ed, ownership moves from the "creator" to the "scheduler."

```cpp
// C++ Simulation
std::unique_ptr<Task> t1 = std::make_unique<Task>();
std::unique_ptr<Task> t2 = std::move(t1); // t1 becomes null, t2 owns the task
```

In the scheduler, we handle a more complex case: Tasks being **Detached**.

### 2. Management Rights: I'm watching you, but I don't necessarily own you.
In `tiny_coro`, the `Task` object holds the **ownership** of the coroutine, while the Scheduler's queues often only hold **management rights (or temporary possession)**.

* **The Difference**:
  * **Ownership**: Has the sole power to decide life or death (calling `delete` or `destroy`).
  * **Management Rights**: Responsible for arranging your work (calling `resume`), but not for killing you.

**Analogy**:
* You own a car (**Ownership**).
* You give the keys to a valet; the valet can drive the car (**Management Rights**).
* Only you can sell or scrap the car; the valet cannot.

In our implementation, we blur this line using **Reference Counting (Ref Count)**: when the last person holding management rights leaves, they trigger the final ownership logic to destroy the object. This is why `task.h` needs a `ref_count`.

üëâ **Core Understanding**: Once a task is `detach()`ed into a queue, the original `Task` object is no longer responsible for its life. The power of life and death now lies in the hands of the **reference count**.

---

**Now that these obstacles are cleared, you have the "ticket" to understand the scheduler's low-level code.**

---

## Chapter 1: The Vessel of the Soul ‚Äî Coroutine Primitives (The Atom)

Before we dive into the architecture, we must define the fundamental unit: the "Task."
In C++20, coroutines are no longer arcane "black magic." The compiler not only allows a function to **suspend** its execution but also enables saving its entire stack frame onto the heap. We need a robust container to hold this "paused function."

### 1.1 What is a `Task`?
Think of it like playing a video game and hitting the "Save" button.
* **Normal Function**: No save feature. You must play until the game ends or you exit (`Return`).
* **Coroutine**: Supports "Save Games" (**Suspend**). The `Task` object acts as that **save file**. It holds a "remote control" called a `coroutine_handle`, which can press "Load Game" (**Resume**) at any time to resume execution.



### 1.2 Key Design: Ownership and Lifecycle
A major pitfall exists: **Who is responsible for destroying this save file?**
In a single-threaded environment, it‚Äôs trivial. But in a multi-threaded scheduler, a task might suspend on Thread A, be resumed by Thread B, and finally be destroyed on Thread C.
* **Atomic Reference Counting**: We need a mechanism similar to `std::shared_ptr`, but implemented with atomic operations to ensure safety across cores.
* **Symmetric Transfer**: When a task completes, we avoid recursive calls (which lead to stack overflow). Instead, we jump directly to the next task in the pipeline.

üëâ **Core Action**:
Review the `Task` implementation. Focus on the `Promise` struct and the `detach()` method. `detach` is the critical bridge that transforms a task from a "temporary stack object" into a "managed pointer" in the scheduling queue.

> üîó **Deep Dive**:
> * [üìñ **Doc: Coroutine Handles and Lifecycle Management**](docs/task.md)
> * [üíª **Code: include/task.h**](include/task.h)

---

## Chapter 2: Micro-Foundations ‚Äî Thread Sleeping and Waking (Parker)

> As we‚Äôve learned, coroutines can suspend and later resume on different threads. But what happens when a thread finds itself without tasks for a brief moment? This chapter explains the synchronization behind that.
> **The Mission**:
> When a **Worker** has no tasks, we shouldn't let it "Spin" and waste CPU cycles; it should go to sleep (**Suspend**).
> However, the ultimate nightmare in concurrent programming is the **"Lost Wakeup"** problem: You decide to sleep, but right before you close your eyes, the alarm goes off. You miss it, and end up sleeping forever.

### 2.1 Why Not Use `std::mutex`?
Performance is our top priority.
Traditional `std::mutex` and `std::condition_variable` are "heavy." They frequently involve kernel-level locking mechanisms, which carry significant overhead.

In C++20, we utilize **`std::atomic::wait`**.
* **Analogy**:
  * **Old Method**: Hiring a doorman (Mutex) to guard the door; every entry and exit requires unlocking and registration.
  * **New Method**: Sticking a "Post-it" note (Atomic) on the door. I only check the note; if the value is wrong, only then do I sleep. Most checks happen in **User Space**, only dipping into the kernel when actual suspension is required.

### 2.2 The Three-State Machine: Solving "Lost Wakeup"
To prevent deadlocks caused by missing a signal, the `Parker` maintains a sophisticated three-state transition:

* **`EMPTY` (0)**: Initial state, no activity.
* **`PARKED` (1)**: The thread is sleeping, or **preparing** to sleep.
* **`NOTIFIED` (2)**: Someone has already signaled you (even if you haven't technically slept yet).



#### ‚úÖ The Solution Provided
1.  Thread B's `unpark` forces the state from `EMPTY` to **`NOTIFIED`**.
2.  Before Thread A sleeps, it performs a **CAS (Compare-And-Swap)** check: "Is the state still `EMPTY`?"
3.  **No!** The state was changed to `NOTIFIED` by B.
4.  The CAS fails; Thread A **cancels its sleep** and immediately turns back to process the new task.

> üîó **Deep Dive**:
> * [üìñ **Doc: Parker Design and C++20 Atomic Wait**](docs/parker.md)
> * [üíª **Code: include/parker.h**](include/parker.h)

---

## Chapter 3: The Memory Guardian (EBR)

> We now know how tasks flow between threads. However, C++ is famous for its manual memory management. While C++ lacks a built-in GC, the industry has long utilized the **EBR (Epoch-Based Reclamation)** algorithm to solve memory safety in lock-free structures.
> **The Mission**:
> Later, we will implement a lock-free `StealQueue`.
> Lock-free queues are fast because they lack Mutexes, but they introduce a fatal risk: **Reclamation Race**.
>
> **Scenario**:
> 1. Thread A is reading Node X (preparing to steal a task).
> 2. Thread B (the Owner) expands the queue, removes old array X, and calls **`delete X`**.
> 3. Thread A continues reading X's memory -> **BOOM! (Use-After-Free)**.
>
> We avoid `std::shared_ptr` here because atomic reference counting causes **Cache Line Bouncing**, which tanks performance. We need a **Zero-Overhead Read** garbage collection mechanism.

### 3.1 What is Epoch-Based Reclamation (EBR)?
EBR is a **Deferred Reclamation** strategy. Its core philosophy: **"As long as a single thread remains in an 'old era,' I will not destroy the 'garbage' belonging to that era."**

It tracks a Global Epoch ($G$) and a Local Epoch ($L$) for each thread.
* **Global Epoch ($G$)**: The current "world time."
* **Local Epoch ($L$)**: The "ticket" time held by a specific thread.
* **Active Flag**: Indicates whether a thread is currently working in a "Critical Section."



### 3.2 The Cycle of Three Generations
For safety, we use three "bins" corresponding to three eras:
1.  **$G$ (Current)**: New garbage is placed here.
2.  **$G-1$ (Previous)**: The era that just passed. Slow threads might still be referencing this. **Dangerous‚ÄîDo not delete.**
3.  **$G-2$ (Safe)**: Since global time has reached $G$, it means all active threads have moved at least to $G-1$. No one can possibly see $G-2$ data anymore. **Safe‚ÄîPurge it!**

### 3.3 The Law of Memory Ordering
When reading `include/ebr.h`, pay close attention to this specific line in `enter()`:

```cpp
// Must use seq_cst (Sequential Consistency)
local->active.store(true, std::memory_order_seq_cst);
```

This is the **Iron Gate** of multi-threaded programming.
* **The Consequence**: Without this, the CPU might reorder instructions, reading the data pointer *before* marking itself as `active`. A GC thread might sweep through, see you aren't "active," and delete the memory while you are mid-read.
* **The Role of `seq_cst`**: It forces the CPU to **Raise your hand (Active) before taking the data**. No exceptions.

> üîó **Deep Dive**:
> * [üìñ **Doc: EBR Generation Rotation and Memory Ordering**](docs/ebr.md)
> * [üíª **Code: include/ebr.h**](include/ebr.h)

---

# Chapter 4: Task Distribution Queues

> We now have our tasks and have solved the most obvious execution issues. Now, we need to ensure these tasks can be processed efficiently. Follow along with this chapter to learn how.
> **Core Mission**:
> The core of a scheduler is "getting tasks to idle CPU cores."
> we have designed a two-tier logistics system:
> 1.  **GlobalQueue (The Warehouse)**: Used to receive tasks submitted externally (e.g., from the `main` function). For absolute safety, we use a mutex here.
> 2.  **StealQueue (The Satchel)**: This is private to each Worker. It must be lightning-fast, almost like accessing CPU registers. For this, we use the **Chase-Lev Lock-Free Algorithm**.

**You now need to read the Chase-Lev Lock-Free Algorithm analysis; jump below:**
[Chase-Lev Algorithm Analysis](docs/Chase_lev.md)

---

### 1. Why a Two-Tier Queue?
* **Multi-Producer Contention (MP)**: External threads (like networking or main threads) may generate tasks simultaneously. If there were only one lock-free queue, multiple producers would cause severe **CAS contention**, tanking performance.
* **Locality**: Tasks generated by the Worker itself (e.g., Coroutine A spawning Coroutine B) should ideally stay with that same Worker, as the data is already in the CPU cache (**Hot Data**).

### 2. StealQueue: The Pinnacle of Lock-Free Programming
`StealQueue` is a **SPMC (Single-Producer, Multi-Consumer)** deque.
* **Owner (The Worker)**: Operates at the **Bottom** (tail). `push` and `pop` usually happen here; it's like taking items from your own satchel‚Äîno atomic operations are needed most of the time.
* **Thief (Other Workers)**: Operates at the **Top** (head). When they are hungry, they try to `steal`, which requires atomic operations.
* **If you haven't read the [Chase-Lev Algorithm Analysis](docs/Chase_lev.md) yet, you must go back and read it now, otherwise the lock-free logic will be very difficult to grasp.**



### 3. Core Code Breakdown

#### A. Preventing False Sharing
```cpp
alignas(64) std::atomic<long> top{0};
alignas(64) std::atomic<long> bottom{0};
```
* **The Problem**: `top` and `bottom` are high-frequency variables. If they sit too close (in the same Cache Line), the Owner modifying `bottom` invalidates the Thief's `top` cache. This is **False Sharing**, causing performance to plummet.
* **The Solution**: `alignas(64)` forces them to occupy their own 64-byte space (typical cache line size) to stay out of each other's way.

#### B. Expansion and EBR
When the queue is full, we must `resize`.
* **The Danger**: When the old `Array` is swapped out, a Thief might still be reading from it.
* **The Strategy**: Call `EbrManager::retire(..., a)`. This throws the old array into the EBR "trash bin," ensuring it's only deleted once everyone has moved on from the old era.

#### C. Dekker‚Äôs Algorithm and SeqCst
The most mind-bending part is in `pop()`:
```cpp
bottom.store(b, std::memory_order_relaxed); // 1. Decrement Bottom first
std::atomic_thread_fence(std::memory_order_seq_cst); // 2. Strong Fence
long t = top.load(std::memory_order_seq_cst); // 3. Then check Top
```
* **Race Condition**: When **only one task remains**, the Owner tries to `pop` and a Thief tries to `steal` at the same time. They will collide.
* **SeqCst Fence**: This implements the classic **Dekker‚Äôs Mutual Exclusion Algorithm**. It forces the CPU to guarantee: **"My modification (Bottom-1) must be visible to others before I can see theirs (Top)."** Omitting this could lead to double consumption or lost tasks.
  *It is highly recommended to read the Dekker‚Äôs Algorithm guide here:* [Dekker's Algorithm Analysis](docs/Dekker.md)

> üîó **Deep Dive**:
> * [üìñ **Doc: Two-Tier Queue Design and Chase-Lev Details**](docs/queue.md)
> * [üíª **Code: include/queue.h**](include/queue.h)

---

# Chapter 5: The Execution Unit ‚Äî The Worker
> We've finally finished the prerequisites; now we just need someone to do the work.
> **Core Mission**:
> We have built the conveyor belts (Queues) and the sleep pods (Parker). Now we need "Workers" to operate them. In `tiny_coro`, a **Worker** is not just a thread; it is a **highly autonomous entity**. It doesn't wait for assignments; it actively hunts for work.

### 1. The Worker‚Äôs Gear
A qualified `Worker` carries four essentials:
1.  **Satchel (`local_queue_`)**: A private lock-free queue. New tasks go here for the fastest "hot" consumption.
2.  **Access Badge (`ebr_state_`)**: Swiped (Enter Epoch) before entering lock-free structures to ensure memory safety.
3.  **Bunk (`parker_`)**: Used to sleep when no work is available, saving CPU power.
4.  **Dice (`rng_`)**: Used to decide whose work to steal.

### 2. A Day in the Life: The Run Loop
The Worker‚Äôs `run()` method is an infinite loop following a strict **Greedy Strategy**:

1.  **Eat Local**: Check `local_queue_`. This is LIFO (Last-In-First-Out); recent tasks are still in cache and execute fastest.
2.  **Share the Global Meal**: If local is empty, check the `Scheduler`'s `global_queue_`.
3.  **Steal from Neighbors**: Everything empty? **Work Stealing** begins!
* Roll the dice (`rng_`) to pick a random colleague and try to steal a task from the top of their queue.
* *Why random?* Math proves random victim selection balances load fastest and avoids the "thundering herd" effect.
4.  **Sleep (Park)**: Still no work? **Crucial Step**: Must exit the EBR critical section (`exit`) before calling `parker_.park()`.
* *Warning*: Sleeping with an active EBR status will stall global memory reclamation, leading to OOM.



### 3. Core Code Highlights
Focus on the design in `include/worker.h`:
* **Delete Copy Constructor**: `Worker(const Worker&) = delete;`. Workers are 1-to-1 with physical threads.
* **Independent RNG**: Each Worker has its own `std::mt19937` to avoid locking contention on a shared random generator.

> üîó **Deep Dive**:
> * [üìñ **Doc: Worker Autonomy and Random Stealing Strategy**](docs/worker.md)
> * [üíª **Code: include/worker.h**](include/worker.h)

---

# Chapter 6: The Scheduler Core (Scheduler & Reactor)
> Finally, we arrive at the heart: the M:N Scheduler. Completing this means you've built a mini version of Go or Tokio's runtime.
> **Core Mission**:
> We assemble all components (Task, Queue, Parker, Poller) into a complete runtime.
> * **Scheduler**: Oversees the whole system and manages the thread pool.
> * **Worker**: Executes tasks and finds work.
> * **Reactor**: Monitors timers and I/O events.

---

### 1. Architecture Overview
The `scheduler.h` file is the central hub connecting three worlds:
1.  **User World**: Users call `spawn(Task)` to submit work.
2.  **Compute World**: `Worker` threads execute CPU-intensive tasks.
3.  **I/O World**: The `Reactor` thread waits for network packets or timers to expire.



### 2. Key Component Analysis

#### A. `Scheduler` (The Foreman)
* **Responsibilities**:
  1.  Initialize `N` `Worker` threads (usually equal to CPU cores).
  2.  Maintain the `GlobalQueue`.
  3.  Start the `Reactor` for I/O.
* **`spawn(Task t)`**: The primary interface. It uses `t.detach()` to transfer ownership to the queue.

#### B. `Worker` (The Laborer)
* **`run_once` De-bouncing**: If no task is caught, don't sleep immediately. **Spin** for a bit; this reduces expensive system call overhead if a task arrives shortly.

#### C. `Reactor` (The Look-out)
* **Cross-platform Poller**: Uses `poller.h` to abstract `epoll_wait` or `kevent`.

### 3. Code Highlights

#### Highlight 1: Unified I/O Callback
The Reactor doesn't need to know what a `Task` is; it only handles `void* udata`. When an event fires, it sends the pointer back to the Scheduler to be spawned.

#### Highlight 2: Asynchronous Sleep (`AsyncSleep`)
When you `co_await sleep_for(sched, 100)`:
1.  Coroutine suspends.
2.  Handle is handed to the Reactor's timer heap.
3.  Worker immediately moves to the next task (**No thread blocking!**).
4.  After 100ms, Reactor pushes the Handle back to a Worker's queue to resume.

> üîó **Deep Dive**:
> * [üìñ **Doc: Scheduler Architecture and Reactor Design**](docs/scheduler.md)
> * [üíª **Code: include/scheduler.h**](include/scheduler.h)
> * [üíª **Poller Documentation**](docs/poller.md)
> * [üíª **Poller Source Code**](include/poller.h)

---

# Chapter 7: The Art of Cooperation ‚Äî CSP Channels (Channel)

> I'm sure you've heard the saying: "Don't communicate by sharing memory; share memory by communicating." That's right, we are going to implement a Go-like channel type here.
> **Core Task**:
> Coroutines often need to pass data between each other.
> * **Primitive Way**: Using global variables + `Mutex`. Prone to deadlocks and high contention.
> * **CSP Way**: Using a `Channel`. It acts like a conveyor belt connecting producers and consumers.
>
> Our goal is to implement a Channel that supports both **Buffered** and **Unbuffered (Rendezvous)** modes, utilizing C++20 black magic to push performance to the limit.

### 1. Core Mechanism: The High-Speed Conveyor Belt
A `Channel` is essentially a thread-safe queue, but it has the added ability to **"suspend"**.
* **Sender**:
  * Buffer not full -> Put data in, return immediately.
  * Buffer full -> **Suspend coroutine**, wait for a slot.
* **Receiver**:
  * Buffer not empty -> Take data, return immediately.
  * Buffer empty -> **Suspend coroutine**, wait for data.

### 2. Performance Optimization Black Magic: `bool await_suspend`
This is one of the most obscure yet powerful features in C++20 coroutines. We use it extensively in `include/channel.h`.

#### Traditional Approach (`void await_suspend`)
1.  Coroutine prepares to suspend.
2.  Enter `await_suspend`, acquire lock.
3.  Discover the buffer actually isn't full (resource available).
4.  Call `scheduler.spawn(self)` to throw itself back into the scheduler.
5.  **Cost**: The current thread must switch to executing another task, and the scheduler will pick this coroutine back up later. This is a complete **Context Switch**, rendering the CPU cache useless (Cache Miss).

#### The Extreme Speed Approach (`bool await_suspend`)
1.  Coroutine prepares to suspend.
2.  Enter `await_suspend`, acquire lock.
3.  **Check Resources**:
  * **Resource Available**: Operate on the buffer directly, then **`return false`**.
  * **Meaning**: "Never mind, I won't suspend, let's keep running."
  * **Result**: The coroutine **resumes execution immediately**, without even switching threads. The instruction pipeline remains unbroken, and data stays in CPU registers. This is an **order-of-magnitude performance boost**.
  * **No Resource**: Store handle in the wait queue, **`return true`**. This is a real suspension.

### 3. Core Code Actions: Fast Path vs Slow Path
Please focus on reading `SendAwaiter` and `RecvAwaiter` in `include/channel.h`.

* **Send Logic**:
  1.  **Someone is waiting (Direct Handoff)**: Stuff data directly to the receiver, wake them up. **I don't even need to enter the buffer**. (`return false` ‚úÖ)
  2.  **No one waiting but space available (Buffer)**: Put into buffer. (`return false` ‚úÖ)
  3.  **Full (Block)**: Get in line, yield CPU. (`return true` ‚õîÔ∏è)

* **Recv Logic**:
  1.  **Buffer has goods**: Take one. If someone is waiting to send, pull them in to fill the hole. (`return false` ‚úÖ)
  2.  **No goods but someone is waiting to send (Capacity=0)**: Grab data directly from the sender. (`return false` ‚úÖ)
  3.  **Empty**: Get in line, yield CPU. (`return true` ‚õîÔ∏è)

### 4. Why does `capacity=0` implement synchronization?
When capacity is 0:
* `buffer.size() < capacity` is always false -> **Never enter the buffer**.
* Sender must wait until `recv_waiters` is not empty (someone to catch) to take the Fast Path.
* Receiver must wait until `send_waiters` is not empty (someone to deliver) to take the Fast Path.
* This forces both parties to "handshake" **(Rendezvous)** at the exact same moment, achieving strong synchronization.

> üîó **Deep Dive & Code Review**:
> * [üìñ **Doc: CSP Model & bool await_suspend Optimization**](docs/channel.md)
> * [üíª **Code: include/channel.h**](include/channel.h)

---

# Chapter 8: The Art of Cooperation ‚Äî AsyncMutex

> In programming, a lock is meant for protection. Unfortunately, while `std::mutex` is safe, it's too "heavy"‚Äîusing it causes the entire coroutine to stop responding. However, we still need a coroutine-level mutex to protect coroutines individually. This chapter will teach you how to implement a coroutine-level mutex.
> **Core Task**:
> In an M:N scheduling model, a physical thread might host thousands of coroutines.
> If you call `std::mutex::lock()` inside a coroutine:
> * **Consequence**: The entire physical Worker thread is suspended. Thousands of coroutines on it stop responding.
> * **Solution**: We need an **AsyncMutex**.
> * **When locked**: It doesn't block the thread; instead, it **suspends** the current coroutine and puts it in a wait queue.
> * **When released**: It takes a coroutine from the queue and throws it back to the scheduler.

### 1. Core Mechanism: Baton Passing
To pursue extreme performance, we adopted the **Baton Passing** technique.

* **Traditional Preemptive Lock**:
  1.  A releases lock (`locked = false`), shouts "Who wants it?".
  2.  B (queueing) and C (just arrived) fight for it simultaneously.
  3.  **Drawback**: Unfair (starvation), and severe Thundering Herd effect.
* **Baton Passing Lock**:
  1.  A releases lock, but **does not set the lock state to false**.
  2.  A directly hands the "key" to B, who is first in line.
  3.  When B wakes up, it naturally assumes it already holds the lock.
  4.  C arrives, sees the lock is still locked, and obediently goes to queue.
  * **Advantage**: Absolutely fair (FIFO), and the woken coroutine doesn't need to CAS loop to grab the lock again.

### 2. Core Code Action: Double-Check
Inside `await_suspend`, there is a critical detail used to prevent **Lost Wakeup**.

```cpp
bool await_suspend(std::coroutine_handle<> h) {
    std::lock_guard<std::mutex> lock(mutex.wait_mtx_); // Small lock protecting internal queue

    // ‚õîÔ∏è Must check lock state again!
    if (!mutex.locked_) {
        mutex.locked_ = true; // Oops, the lock holder just released it!
        return false;         // Lucky day, no need to suspend, enter directly.
    }

    // It is indeed occupied, enqueue and suspend
    mutex.waiters_.push(h);
    return true;
}
```
* **Scenario**:
  1.  Coroutine A calls `await_ready`, finds lock occupied, returns false (preparing to suspend).
  2.  **At this exact split second**, lock holder B releases the lock.
  3.  Coroutine A enters `await_suspend`. If it doesn't check again, A will fall asleep on a lock that is **actually free**, leading to a deadlock.

### 3. Why is there another `std::mutex` inside?
In the `AsyncMutex` code, you will see a `wait_mtx_`.
* **Question**: Didn't you say we can't use `std::mutex`?
* **Answer**:
  * Where we **cannot** use it: Protecting **user business logic** (potentially taking milliseconds).
  * Where we **can** use it: Protecting the **lock's internal wait queue** (taking only nanoseconds).
  * As long as the critical section is extremely short, the overhead of `std::mutex` is negligible.

### 4. RAII Safeguard: `ScopedLock`
Writing `lock/unlock` manually in coroutines is prone to accidents (e.g., throwing exceptions in the middle, or early `co_return`).
We provide `ScopedLock`, utilizing C++'s destructor mechanism to ensure the lock is always released.
```cpp
{
    auto guard = co_await mutex.scoped_lock();
    // ... Critical Section ...
} // Out of scope, automatically unlock
```

> üîó **Deep Dive & Code Review**:
> * [üìñ **Doc: Baton Passing & Lost Wakeup Fix**](docs/async_mutex.md)
> * [üíª **Code: include/async_mutex.h**](include/async_mutex.h)

---

# Chapter 9: Extreme Micro-Management ‚Äî SpinLock

> The concept of a SpinLock is actually quite simple. The only reason it exists is that we need extreme performance. It's essentially not a lock, but rather hogging the CPU without letting go. Therefore, you must never assume that because SpinLocks guarantee performance, you can spin everywhere‚Äîthis will turn your CPU into a microwave.
> **Core Task**:
> At the lowest level of high-concurrency systems, we care about **nanosecond (ns)** level performance.
> * **Scenario**: I need to modify a flag, taking only 5ns.
> * **std::mutex**: If lock not acquired, enter kernel sleep, context switch occurs. Cost approx 2000ns. **Huge loss!**
> * **SpinLock**: If lock not acquired, I idle (Spin) on the CPU, staring at it. Usually, the lock is released after 10ns. **Big win!**

### 1. Core Mechanism: Busy Waiting
The essence of a SpinLock is: **"I'm standing right at the door, going nowhere, until you open it."**
It applies to scenarios where the **critical section is extremely short** and **contention is not intense**. In `tiny_coro`, it is typically used to protect low-level operations that only take a few instructions to complete.

### 2. Performance Optimization Black Magic: TTAS & Cache Line
Ordinary SpinLocks (`while(exchange(...))`) cause severe performance issues. We use the **TTAS (Test-Test-And-Set)** algorithm.

#### ‚ùå Naive TAS (Test-And-Set)
```cpp
while (lock_.exchange(true)) {} // Frantic grabbing
```
* **Consequence**: All cores frantically execute `LOCK` instructions (RMW operations).
* **Phenomenon**: **Bus Storm**.
* **Physical Layer**: The Cache Coherency Protocol (MESI) causes the **Cache Line** containing the lock variable to "bounce" wildly between different CPU cores. Bus bandwidth gets saturated, performance collapses.

#### ‚úÖ Optimized TTAS (Test-Test-And-Set)
```cpp
while (true) {
    if (!lock_.exchange(true)) return; // 1. Try to grab once

    // 2. Only if grab fails, enter read-only loop
    while (lock_.load(std::memory_order_relaxed)) {
        _mm_pause(); // 3. CPU pause instruction
    }
}
```
* **Principle**:
  * When the lock is occupied, everyone executes `load` (read operation).
  * **Read operations can share cache (Shared State)**. Everyone can read the value from their own L1 cache without hitting the bus.
  * Only when the lock holder releases (`store false`), invalidating the cache, does everyone rush out at the same moment to try `exchange`.
* **Effect**: Reduces bus traffic by over 90%.

### 3. CPU Breather: `_mm_pause`
In the inner loop, `_mm_pause()` (x86) or `yield` (ARM) must be added.
* **Pipeline Optimization**: Tells the CPU "I am busy-waiting". The CPU will clear speculative instructions in the pipeline, avoiding **Pipeline Flush** penalties when the lock state changes.
* **Hyper-Threading Friendly**: If your CPU is hyper-threaded (one physical core, two logical cores), `pause` yields execution resources to the other thread on the same core.

### 4. Memory Order Selection
* **Lock (`acquire`)**: After I get the lock, I must be able to see all modifications made by the previous person inside the critical section.
* **Unlock (`release`)**: Before I release the lock, my modifications inside the critical section must be visible to the next person.
* **Spin Check (`relaxed`)**: I'm just checking if anyone is at the door; I don't need to establish a synchronization relationship, the faster the better.

> üîó **Deep Dive & Code Review**:
> * [üìñ **Doc: TTAS Algorithm & Cache Line False Sharing**](docs/spinlock.md)
> * [üíª **Code: include/spinlock.h**](include/spinlock.h)

---
# Chapter 10: Perception and Connection ‚Äî‚Äî Time Management (Timer)
> Our rough scheduler is complete, and it's very good, but it can only handle current tasks and cannot handle future tasks. However, in reality, we are always planning what we should do in the future. Now, we need our scheduler to be able to accomplish future tasks as well.
> **Core Task**:
> The scheduler must not only handle tasks that need to be done "right now" but also tasks to be done in the "future".
> For example, `co_await sleep_for(10s)`.
> We need a data structure to orderly manage these "time bombs," ensuring they are triggered the instant their time is up.

### 1. Why can't we use system time?
In `include/timer.h`, we mandate the use of `std::chrono::steady_clock`.
* **Wall Clock (`system_clock`)**: The clock on the wall. If you manually change the system time, or if NTP automatically calibrates the time, it will **jump**.
  * *Disaster Scenario*: Coroutine A sleeps for 10 seconds. Suddenly, the system time is set back by 1 hour. Coroutine A will actually end up sleeping for **1 hour, 0 minutes, and 10 seconds**!
* **Monotonic Clock (`steady_clock`)**: A stopwatch in your hand. It only cares about **how much time has passed since boot**. Time always increases monotonically and never turns back.
  * *Correct Solution*: No matter how the system time changes, 10 seconds is just 10 seconds.

### 2. The Magic of Min-Heap
We frequently need to do two things:
1.  **Insert**: A new coroutine wants to sleep for 5 seconds.
2.  **Query**: Who is the next coroutine that needs to wake up? How much longer until then?

**Data Structure Selection**:
* **Array/Linked List**: Insertion is $O(1)$, but querying the minimum value is $O(N)$. Traversing all timers every time is too slow.
* **Min-Heap (`priority_queue`)**: Insertion is $O(\log N)$, querying the minimum value is $O(1)$. Perfect.

### 3. Core Code Action: Type Erasure
```cpp
struct Timer {
    TimePoint expiry;
    std::coroutine_handle<> handle; // <--- Focus here
};
```
* **Universal Remote (`coroutine_handle<>`)**:
  * The `Timer` doesn't need to know what the coroutine returns (`Task` or `Generator`), nor does it need to know the data inside it.
  * It only needs a `void*` pointer to call `resume()` and wake it up when the time comes.
* **Zero Overhead**:
  * Compared to `std::function` (32+ bytes, potential memory allocation), `coroutine_handle<>` is only **8 bytes** (pointer size) and involves no extra memory allocation. This is crucial for massive scheduled tasks (like C10K scenarios).

### 4. Heap Comparison Logic
```cpp
// To turn priority_queue (default Max-Heap) into a Min-Heap
// We use it with std::greater and need to overload the > operator
bool operator>(const Timer& other) const {
    return expiry > other.expiry; // Later time counts as "larger", sinking to the bottom
}
```
* The logic here is a bit twisty: `std::priority_queue` puts the "large" elements at the top. If we want the "early" ones (small time values) at the top, we have to tell it: "Later is larger."

> üîó **In-depth Learning & Code Reference**:
> * [üìñ **Documentation: TimePoint Selection and Min-Heap Implementation**](docs/timer.md)
> * [üíª **Code Implementation: include/timer.h**](include/timer.h)

---

# Chapter 11: Perception and Connection ‚Äî‚Äî Network I/O (Socket)
> In fact, our scheduler is already excellent, but currently, it can only handle parallel computational tasks (like matrix calculations). We need to enable it for broader application scenarios.
> **Core Task**:
> Coroutines are very suitable for handling I/O-intensive tasks (and computation-intensive tasks too), because most of the time is spent **waiting**.
> * **Traditional Synchronous I/O**: Call `read` -> No data -> Thread blocks (Sleeping). **Wastes thread resources.**
> * **Coroutine Async I/O**: Call `co_await read` -> No data -> Coroutine suspends (Suspended), thread goes to do something else. **Squeezes the CPU dry.**
>
> We need to encapsulate Linux Socket APIs and turn them into **Awaitable** objects.

### 1. Core Mechanism: Non-blocking + Reactor
In `include/socket.h`, all Sockets are set to **`O_NONBLOCK`** upon creation.
* **This means**: When you call `read`, if the kernel buffer has no data, it won't block but will immediately return `-1` and set `errno = EAGAIN` (Try again later).
* **Role of Reactor**:
  * When `read` returns EAGAIN, the coroutine doesn't error out but tells the Reactor: "I'm busy, suspending for now. Wake me up when this Socket becomes **readable**."

### 2. The Awaitable Three-Step Dance
This is the standard flow for C++20 Coroutines interacting with a Reactor. Taking `AsyncReadAwaiter` as an example:

#### Step 1: `await_ready` (Fast Path)
* **Attempt Direct Read**: Call `read` once first.
* **Success**: Data is already in the kernel buffer! Return `true` immediately. **The coroutine does not suspend and continues execution.**
  * *Performance Key*: This avoids the expensive "Suspend -> Register epoll -> Wake up" flow, drastically reducing latency.
* **Failure (EAGAIN)**: Really no data. Return `false`, proceed to the next step.

#### Step 2: `await_suspend` (Suspend & Register)
* **Save Handle**: Store the current coroutine handle (`coroutine_handle`).
* **Register Event**: Call `reactor_->register_read(fd, handle)`. Tell the Reactor: "Wait for data on this fd."
* **Keep-Alive**: `ref_count.fetch_add(1)`. Prevent the coroutine from being accidentally destroyed while waiting.

#### Step 3: `await_resume` (Wake Up)
* **Reactor Notification**: Data arrived! Reactor calls `handle.resume()`.
* **Read Again**: Coroutine resumes, calls `read` again. This time it will definitely read data (or a disconnect).
* **Return Result**: Return the number of bytes read to the user.

### 3. Core Code Action: The Write Deadlock Trap
Please note the implementation of `AsyncWriteAwaiter`:
```cpp
// ‚ùå Wrong Way: Wait for readable
// reactor_->register_read(fd_, h); 
// Please do not assume you must read before writing. Once read is done you can write? No sequence order. 
// In the eyes of the computer, these are two independent events.
// ‚úÖ Correct Way: Wait for writable
reactor_->register_write(fd_, h.address());
```
* **Scenario**: Sending a 100MB file. Kernel Send Buffer is full.
* **Deadlock**: If you register `register_read`, you are waiting for the client to send data, while the client is waiting for you to finish sending the file. Deadlock.
* **Correct Solution**: Register `register_write`. When the kernel sends out a portion of the data from the buffer, the buffer becomes empty (Writable), and the Reactor will wake you up to continue sending.

> üîó **In-depth Learning & Code Reference**:
> * [üìñ **Documentation: Awaitable Implementation and Lifecycle Management**](docs/socket.md)
> * [üíª **Code Implementation: include/socket.h**](include/socket.h)

---

# Chapter 12: Business Online ‚Äî‚Äî HTTP Server
> **Core Task**:
> We already possess a powerful asynchronous runtime. Now, we are going to use it to build a real application: A Web Server.
> Our goal is not to write a toy, but a high-performance server capable of arm-wrestling with Nginx.
> To achieve this, we must squeeze every drop of performance from the CPU and memory, adopting **Zero-Copy** and **Streaming** technologies.
**Note: My implementation here is very rudimentary, merely to establish network connectivity for testing with `wrk`. You should write better HTTP methods.**
---

### 1. Zero-Copy HTTP Parser (`HttpParser`)
In network programming, the slowest part is not network transmission, but **memory copying**.
The traditional approach is: `socket.read()` -> `new string` -> `parse` -> `new string(header)`.
Every step involves allocating memory and copying data.

#### 1.1 The Magic of `std::string_view`
In our `HttpRequest` structure, all fields (Method, Path, Headers) are `std::string_view`.
* **Principle**: `string_view` is just a pointer and a length. It **does not own** memory; it is merely a "window" into the original receive buffer.
* **Effect**: No matter how long the URL or how big the Header, the parsing process involves **0 memory allocations** and **0 data copies**.

#### 1.2 Stack-Allocated Header Array
We use the `picohttpparser` library, combined with a stack array `struct phr_header headers[32]`.
* **Fast Path**: 99% of request Headers count fewer than 32. We completely avoid the heap allocation overhead of `std::vector`.

> üîó **In-depth Learning & Code Reference**:
> * [üìñ **Documentation: Zero-Copy Parsing Principle**](docs/http_parser.md)
> * [üíª **Code Implementation: include/http/http_parser.h**](include/http/http_parser.h)

---

### 2. Streaming File Transmission (`HttpServer`)
If a user wants to upload a 10GB file, what should the server do?
* **Wrong Way**: Read 10GB into `std::string` in memory, then write to disk. **Memory Explosion (OOM)**.
* **Correct Way**: Streaming. Read a bit, write a bit.

#### 2.1 Coroutine Streaming Upload (`receive_to_file`)
This is the best demonstration of coroutine power.
```cpp
Task receive_to_file(...) {
    char buf[8192]; // Fixed 8KB memory
    while (total < length) {
        // 1. Async Read: Yield CPU to wait for network packets
        ssize_t n = co_await socket_.read(buf, 8192);
        
        // 2. Sync Write: OS PageCache is extremely fast
        file.write(buf, n);
    }
}
```
* **Clear Logic**: The code is written as simply as a synchronous loop, with no callback hell.
* **Powerful Performance**: No matter how large the file, memory usage is constant at **8KB**. While waiting for network packets, the Worker thread goes off to handle other requests, keeping CPU utilization extremely high.

#### 2.2 Response Sending Optimization (`send_response`)
When sending an HTTP response, we utilize the idea of `writev` (Vectorized I/O):
1.  **Header**: Assembled into a string and sent.
2.  **Body**: Directly send the memory block pointed to by `string_view` (potentially a mmap-ed file).
    This again avoids the massive overhead of copying the Body content onto the end of the Header string.

> üîó **In-depth Learning & Code Reference**:
> * [üìñ **Documentation: Streaming Processing and Business Logic Encapsulation**](docs/http_server.md)
> * [üíª **Code Implementation: include/http/http_server.h**](include/http/http_server.h)

---

# Chapter 13: Advanced Topics: Preemptive Scheduling ‚Äî From "Voluntary" to "Rule of Law"

> **Preface**:
> Up until now, our `tiny_coro` has been a **Cooperative Scheduler**.
> * **Cooperative**: Coroutines must actively call `co_await` or `yield` to relinquish the CPU. If a coroutine writes a dead loop like `while(true) { i++; }`, it will permanently hog the Worker thread, causing other tasks to starve.
> * **Preemptive**: The scheduler possesses a "God view" and "Enforcement Power". If a task runs for too long (e.g., exceeding 10ms), the scheduler will forcibly pause it and switch to the next task.
>
> Industrial-grade language runtimes (such as Go 1.14+, Erlang, Java Virtual Threads) usually introduce preemption mechanisms.



---

## 1. The Principle of Preemption: How to Interrupt a Running Function?

Implementing preemption in C++ User Space is extremely difficult because we do not have interrupt privileges like the operating system kernel does. There are generally three schools of thought:

### A. Signal-Based ‚Äî Go (Early versions) / Linux
This is the most hardcore method.
1.  **Register Signals**: The scheduler registers `SIGURG` or `SIGPROF` signal handlers for each Worker thread.
2.  **Scheduled Sending**: A monitoring thread (Monitor) sends a signal to the Worker running a task every 10ms.
3.  **Interrupt Context**: The OS pauses the Worker's current execution flow and forces a jump to the Signal Handler.
4.  **Modify Context**: Inside the signal handler, modify the Worker thread's `RIP` (Instruction Pointer) and `RSP` (Stack Pointer) to point to the scheduler's `yield` function.
5.  **Resume Execution**: When signal handling ends, the Worker does not return to the task code it was just running, but is "forced" to jump to `yield`, thereby relinquishing the CPU.



### B. Compiler Instrumentation ‚Äî Go (1.14+) / Rust (Experimental)
This is the preferred choice for modern runtimes, with minimal performance loss.
1.  **Function Prologue Check**: The compiler automatically inserts a piece of assembly code at the beginning (Prologue) of every function during compilation.
2.  **Check Flags**: This code checks a global (or TLS) variable called `stack_guard`.
3.  **Trigger Preemption**:
* Normally, the check passes, and execution continues.
* When the scheduler wants to preempt a G (Goroutine), it sets that G's `stack_guard` to an illegal value.
* When G calls any function next, the check fails, triggering the `morestack` logic, which in turn calls the scheduler's `schedule()` to yield.
4.  **Advantages**: Does not require complex signal handling and can occur at any function call boundary.



### C. Safepoint Polling ‚Äî C++20 Feasible Solution
In C++20, we cannot easily modify the compiler's code generation logic. However, we can simulate instrumentation:
1.  **Manual Instrumentation**: Require users to manually insert `co_await yield_if_needed();` in time-consuming loops.
2.  **Time Slice Check**:
    ```cpp
    struct YieldIfTimeout {
        bool await_ready() {
            // Check if the current task has run for more than 10ms
            return (now() - task_start_time) < 10ms;
        }
        void await_suspend(...) { ... } // Suspend
        void await_resume() { task_start_time = now(); } // Reset time
    };
    ```

---

## 2. The Cost of Preemptive Scheduling

Preemption is not a free lunch; it brings massive complexity:

1.  **Data Race**:
* In cooperative scheduling, you know coroutines only switch at `co_await`, so code between two `await` points is "atomic" and doesn't need locking.
* In preemptive scheduling, a coroutine can be cut away at **any** line of code. This means all shared data access must be locked (Mutex/Atomic), which significantly increases code complexity and performance overhead.

2.  **Context Switch Overhead**:
* The signal interruption method involves kernel mode switching, which has high overhead.
* The compiler instrumentation method is fast, but it increases binary size and slightly reduces function call speed.

3.  **GC Safepoint**:
* For languages with GC (like Go/Java), preemption must occur at a "Safepoint" (a moment where pointer positions on the stack are known), otherwise, the GC scan will fail.

## 3. Conclusion: Why Did tiny_coro Choose Cooperative?

For a language like C++ that has no GC and pursues extreme performance, **Cooperative Scheduling** is usually the better solution:
* **Performance**: No interrupt overhead, no instrumentation checks.
* **Controllable**: Developers have complete control over switching timing.
* **Simplicity**: No need to handle complex signal masks and instruction pointer modifications.

If your business code does run the risk of infinite loops, the best practice is to **standardize code writing** (add `co_await yield()` inside loops) rather than introducing heavy preemption mechanisms.

---
## Chapter 14: Future Directions ‚Äî‚Äî Start Your Engines

All components are ready. It is time to assemble them.

### 14.1 The Complete Lifecycle
The flow of a request:
1.  **Main**: `Scheduler` starts, creates Worker threads.
2.  **Listener**: `TcpListener` listens on port 8080 inside the Reactor.
3.  **Accept**: A new connection arrives, Reactor wakes up the Listener.
4.  **Spawn**: Listener creates a new coroutine `handle_client` and throws it into the `GlobalQueue`.
5.  **Schedule**: A Worker grabs the task and begins execution.
6.  **I/O**: The task reads/writes the Socket, suspends while waiting, and the Worker goes off to do other things.
7.  **Done**: The task ends, `Task` reference count hits zero, and it is automatically destroyed.



### 14.2 Hands-on Moment
Now, you can create a `main.cpp`, include all headers, and write your own high-performance server.

> üîó **Reference Example**:
> * [simple_http_web.cpp](src/simple_http_web.cpp)
    > This itself is just an example used by the author to test QPS, so it likely has no reference value for structure.
---

## üéì Epilogue: What Have You Learned?

Congratulations! You have just completed an **Odyssey of Systems Programming**.
Through this project, you have mastered:
1.  **C++20 Coroutines**: No longer magic, but controllable state machines.
2.  **M:N Scheduling**: Understood the core principles behind Go and Rust runtimes.
3.  **Lock-Free**: Learned how to dance on the blade with atomic operations and memory barriers.
4.  **Reactor Pattern**: Understood the event-driven models of Nginx and Node.js.

### üöÄ Where to Next?
Your scheduler is already strong, but it can be stronger:
* **Challenge 1**: Implement **LIFO/FIFO Switching during Work-Stealing** (Improve cache locality).
* **Challenge 2**: Add **Select** functionality to `Channel` (Listen to multiple channels simultaneously like Go).
* **Challenge 3**: Support **SSL/TLS** (Add an encryption layer to Socket).
* **Challenge 4**: Add non-blocking Read/Write. Currently, reading and writing rely on `cout` and `cin`, which drags down the whole system.
* **Challenge 5**: Try to write a DSL yourself to implement preemptive coroutines (Of course, this is extremely difficult for anyone).

**Now, go write code.**
**The code is the truth.**

---

**(End of Book)**